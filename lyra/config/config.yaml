# Lyra RNA Secondary Structure Prediction Configuration

# Weights & Biases Configuration
wandb:
  enabled: true
  entity: null  # Set to your wandb team name, or leave as null for personal account
  project: "lyra-rna-contact-prediction"
  run_name: null  # Set to null for auto-generated name
  tags:
    - "rna"
    - "contact-prediction"
    - "transformer"
  notes: "RNA contact map prediction with Lyra model"

# Data paths
data:
  csv_path: "lyra/dataset.csv"
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  random_seed: 42

# Data loading
dataloader:
  batch_size: 32  # Increased from 16 for better GPU utilization
  num_workers: 4  # Parallel data loading (4 CPU cores)
  pin_memory: true
  prefetch_factor: 2  # Prefetch 2 batches per worker
  persistent_workers: true  # Keep workers alive between epochs
  deduplicate: true  # Prevent data leakage by splitting on unique sequences
  augment: true  # Enable data augmentation for training
  augment_prob: 0.5  # Probability of applying augmentation

# Model architecture
model:
  model_dimension: 256  # Increased from 128 for better capacity
  pgc_configs:
    - [2.0, 3]  # (expansion_factor, num_layers)
    - [2.0, 2]
  num_s4: 6  # Increased from 4 for better capacity
  d_input: 4  # One-hot encoding for A, C, G, U
  dropout: 0.2
  prenorm: true
  final_dropout: 0.2

# Training
training:
  num_epochs: 50
  learning_rate: 0.001
  weight_decay: 0.0001
  grad_clip: 1.0
  early_stopping_patience: 10

  # Performance optimizations
  use_amp: true  # Automatic Mixed Precision (faster training, less memory)
  gradient_accumulation_steps: 1  # Accumulate gradients over N batches

  # Loss function
  pos_weight: 5.0  # Reduced from 10.0 for deduplicated data

  # Learning rate scheduler with warmup
  warmup_epochs: 3  # Number of epochs for linear warmup
  scheduler:
    type: "ReduceLROnPlateau"
    mode: "max"
    factor: 0.5
    patience: 5
    min_lr: 0.00001

# Checkpointing
checkpoint:
  save_dir: "lyra/models/checkpoints"
  save_best_only: false  # Save periodic checkpoints
  save_interval: 5  # Save checkpoint every N epochs
  monitor: "val_f1"  # Metric to monitor for best model
  history_file: "training_history.csv"  # CSV file for loss/metrics tracking

# Device
device: "cuda"  # "cuda" or "cpu"

# Logging
logging:
  log_interval: 100  # Log every N batches
  verbose: true
